---
layout: blog
authors: [ivagljiva, FlorianTrigodet]
title: Metabin refinement and population genetics using human tongue metagenomes
excerpt: "A multi-purpose tutorial based on human oral microbiome data"
date: 2025-03-05
tags: [metagenomics, binning, population genetics, hands-on, beginner]
comments: true
---

Real world datasets are complicated, and it can be difficult to accurately capture this in tutorials due to time and space constraints. The infamous [Infant Gut Tutorial](https://merenlab.org/tutorials/infant-gut/) covers the basics of both binning and population genetics on an extremely simple dataset, which is great for learning but the process and results are not necessarily representative of what one would actually end up doing/seeing in a large-scale 'omics study. Take binning, for example -- if you have dozens of large co-assemblies in your dataset, you won't have the time to do manual binning. And microbial communities containing hundreds of populations rarely show perfectly clean, easy-to-bin patterns of sequence composition and differential coverage. As for population genetics, our existing tutorial shows you how to do the thing, but leaves out some context for the question and important parameter decisions along the way.

With that in mind, we created this tutorial to show a more realistic example of how to work on both metagenomic binning and population genetics in anvi'o, using real-world data from the human oral microbiome as generated in the paper ["Functional and genetic markers of niche partitioning among enigmatic members of the human oral microbiome"](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02195-w) by Shaiber et al (2020). Our goal is to show you the following:

- How to run a read recruitment workflow from many samples to a co-assembly, to generate differential coverage data for binning as well as variant data for population genetics
- How to manually refine a 'metabin' that was automatically generated by Concoct and encompasses several microbial populations into a few smaller yet high-quality bins
- How to set some key parameters in a population genetics analysis that answers a (somewhat) realistic research question

Of course, we are still constrained a little bit by the time and computational resources allowed by a typical tutorial session. We will be working with a smallish co-assembly that is entirely visualizable in the anvi'o interactive interface (unlike many real-world datasets). And it is unlikely that we will actually run the read recruitment workflow in a tutorial setting (but we will give you all the tools to do so in case you want to try it at home).

Without further ado, let's begin :)

## Our dataset and questions

We are taking advantage of the [publicly-available data](https://merenlab.org/data/#niche-partitioning-in-the-human-oral-cavity) from Shaiber et al 2020. This study described a time-series of tongue and plague metagenomes taken from several human individuals, most of which were paired in male-female couples. This sampling strategy is particularly effective for metagenomic binning for the following reasons. First, having multiple samples from a single individual enabled the authors to do co-assembly, which effectively increases the coverage and likelihood of catching low-abundance populations (at the cost of increasing the overall complexity, which can break assembly algorithms). Second, we know that similar populations likely occur across different individuals and it therefore makes sense to map multiple individuals to a given co-assembly to leverage more differential coverage signals -- this was not the strategy used in the Shaiber et al 2020 paper, but it will work quite well for our purposes today.

Here are the questions we want to answer using this dataset:

1. What microbial populations can we recover high-quality metagenome-assembled genomes (MAGs) for by refining 'metabins'?
2. Do the individuals in a couple share similar populations in their oral microbiomes? To put it another way -- can the population variants specific to a given individual help us distinguish between the couples?

In order to answer these questions at a smaller scale, we will be using a subset of the data from the 2020 study - a co-assembly of 5 tongue samples from the time-series of a single individual (the sample named `T-B-M` in the original paper), plus 20 tongue metagenomes to recruit reads from. These include the 5 samples used to make the co-assembly and 5 each from three other individuals: `T-A-F`, `T-A-M`, and `T-B-F`.

For the purposes of this tutorial, we will use the co-assembly that was already generated by the study authors. We will start our binning journey from the results of the automatic binning tool [CONCOCT](https://github.com/BinPro/CONCOCT) ([Alneberg et al 2014](https://www.nature.com/articles/nmeth.3103)) that was already run on the co-assembly. 

In fact, CONCOCT was run twice -- once by specifying `-c 10` to force the tool to create exactly 10 'metabins' (each containing multiple microbial populations), and once without enforcing a number of bins (in which case the tool would try to group sequences from exactly one population in each bin). The 'metabin' strategy is extremely useful for partitioning your large metagenomic datasets into manageable chunks that can be manually refined, and is described elsewhere in [a blog post by Tom and Meren](https://anvio.org/blog/constrained-binning/).

## The mapping workflow

If all we wanted to do was bin refinement, a read recruitment step wouldn't really be necessary here. After all, the automatic binning results were already generated using existing read recruitment results where the 5 time-series samples were mapped against their co-assembly. So we could have just taken the existing, publicly-available contigs and profile databases from [this Figshare](https://figshare.com/articles/dataset/Anvi_o_profiles_per_individual/12217802) and done our bin refinement on that. 

But we want more than that. To answer question (2), we also want to see how the tongue-associated populations from the individual `T-B-M` differ from the tongue-associated populations in the three other individuals (which means we need mapping information from 15 additional samples). And if we are going to map additional samples to the co-assembly anyway, might as well use them to help guide our bin refinement process.

This is probably not a step that you want to run on a laptop. We did it on our high-performance computing cluster (HPC), using the snakemake workflow for metagenomics implemented in `anvi-run-workflow`. We will show you how we set this workflow up, but there is no need for you to run it yourself - you will be able to download the output of the workflow that is required for the rest of the tutorial in the next section.

<details markdown="1"><summary>Show/Hide You want to run the mapping workflow yourself? Here is how to get the short read data. </summary>

To get the metagenome samples, open the [SRA Run Selector for the BioProject PRJNA625082](https://www.ncbi.nlm.nih.gov/Traces/study/?query_key=2&WebEnv=MCID_67c08dce0e84b53f0ea4df2a&f=env_medium_sam_ss%3An&o=acc_s%3Aa) in your favorite web browser. In the filters on the left side, filter for 'host_tissue_sampled' = 'Tongue' and 'Assay_Type' = 'WGS' to get all the tongue samples. Then downloaded the metadata table as a CSV. You can then select only the time-series samples from our 4 individuals of interest by running the following code on the metadata table in your terminal:

```
# keep only accession and sample name
cut -d ',' -f 1,45 Oral_Microbiome_SraRunTable.csv | tr ',' '\t' > oral_samples.txt
# search for sample names of interest and extract to a new file
for n in T_B_M T_B_F T_A_F T_A_M; do \
  grep $n oral_samples.txt >> oral_samps_to_download.txt ; \
done
# extract only the accession for the samples of interest
cut -f 1 oral_samps_to_download.txt > SRA_accession_list.txt
```

This creates a file of SRA accession numbers that becomes the input to the SRA download workflow. You should put that `SRA_accession_list.txt` file wherever you want to download those samples (for instance, on your HPC), and then get a default config file for the workflow:

```
anvi-run-workflow -w sra_download --get-default-config download.json
```

Feel free to open that config and change the parameters (for instance, number of threads) for each rule according to the computational resources available to you.

When you are ready to download the samples, running the workflow is as simple as this:

```
anvi-run-workflow -w sra_download -c download.json
```

If you are running on a cluster, you may want to use the `--additional-params` (or `-A`) flag to pass cluster-specific scheduler commands and resource limits to snakemake using flags such as `--cluster`, `--resources`, `--jobs`, etc.

Once the workflow is done, you should have a folder of FASTQ files for 20 metagenome samples. And then you'll be ready to run the mapping workflow!

</details>

So, how did we run the mapping workflow?

First, we got our reference metagenome co-assembly for individual T-B-M by downloading the corresponding contigs and profile database from [this Figshare link](https://figshare.com/articles/dataset/Anvi_o_profiles_per_individual/12217802). Then, we extracted the FASTA file of the contigs, as well as both existing CONCOCT collections that were already stored in the profile database:

```
# extract le data
tar -xvf T-B-M.tar.gz

# update the dbs to match your current anvi'o version
anvi-migrate --migrate-safe T-B-M/*.db

# get our reference FASTA file
anvi-export-contigs -c T-B-M/CONTIGS.db -o T_B_M-contigs.fa

# export the metabin collection
anvi-export-collection -p T-B-M/PROFILE.db -C CONCOCT_c10 -O CONCOCT_c10

# export the standard CONCOCT collection
anvi-export-collection -p T-B-M/PROFILE.db -C CONCOCT -O CONCOCT
```

We will use the FASTA file of the co-assembly contigs as the reference in our mapping workflow. The mapping workflow will create a new contigs database out of this FASTA file, and associate the mapping data to that new contigs database via a new (merged) profile database containing the mapping info from all 20 of our samples. Because we'll get a fresh profile database that won't have any of the existing collection information in it, we will need to import the collection data (using the collection files we just exported) so that we can do our bin refinement from there. Luckily, because we exported our reference FASTA directly from the contigs database, the split names in the exported collection files will match to the names in the new contigs database. But we are getting ahead of ourselves. First we actually have to run the workflow.

On our HPC, we navigated to the folder where we downloaded the 20 metagenome samples that we want to map against this co-assembly. We created a samples-txt file that describes the paths to each of the samples (you can see the format [here](https://anvio.org/help/main/artifacts/samples-txt/)). Then we generated a fasta-txt file with the path to our reference co-assembly FASTA, and we got a default config file for the metagenomics workflow:

```
echo -e "name\tpath\nT_B_M\tT_B_M-contigs.fa" > fasta.txt
anvi-run-workflow -w metagenomics --get-default-config map.json
```

After modifying the config file a bit to turn off unnecessary rules (like functional annotation) and increase the number of threads for each rule as much as our cluster could handle, we did a dry run (using the `-n` dry run flag and `-q` quiet flag passed directly to snakemake using `-A` or `--additional-params`) to see what the workflow would actually run:
```
anvi-run-workflow -w metagenomics -c map.json -A -n -q
```

And it showed us the following list of jobs:
```
job                                  count
---------------------------------  -------
annotate_contigs_database                1
anvi_gen_contigs_database                1
anvi_init_bam                           20
anvi_merge                               1
anvi_profile                            20
anvi_run_hmms                            1
anvi_run_scg_taxonomy                    1
bowtie                                  20
bowtie_build                             1
gen_qc_report                            1
gzip_fastqs                             40
import_percent_of_reads_mapped          20
iu_filter_quality_minoche               20
iu_gen_configs                           1
metagenomics_workflow_target_rule        1
samtools_view                           20
total                                  169
```

Everything looked okay in the dry run. We have 20 samples, so it makes sense that we are doing all the sample-specific jobs 20 times (the only exception, `gzip_fastqs`, works individually on R1 and R2 files, so it runs twice per samples).

We very confidently started the workflow using the command:

```
anvi-run-workflow -w metagenomics -c map.json
```

And after it finished successfully, we took the resulting contigs database and merged profile database, and we imported the original collections of CONCOCT bins that we extracted previously:

```
anvi-import-collection -p PROFILE.db -C CONCOCT_c10 CONCOCT_c10.txt -c T_B_M-contigs.db
anvi-import-collection -c T_B_M-contigs.db -p PROFILE.db -C CONCOCT CONCOCT.txt
```

These are the databases that were put into the datapack for this tutorial, which you can download in the next section.

## Refining CONCOCT metabins

First, let's download the tutorial datapack. Just so you know, the archived databack is about half a gigabyte in size, and once unpacked it will take up ~1.6Gb of space on your computer. If you are okay with that, here are the download commands:

```
wget https://figshare.com/ndownloader/files/52695236 -O BINNING_POPGEN_TUTORIAL.tar.gz
tar -xvf BINNING_POPGEN_TUTORIAL.tar.gz && cd BINNING_POPGEN_TUTORIAL/
```

You should now be inside the datapack directory on your terminal. The directory should contain 3 databases -- the contigs db containing the co-assembly of the 5 time-series tongue metagenomes from one individual (`T-B-M`), the profile database containing the mapping data from 20 tongue metagenome samples (from the same `T-B-M` individual as well as three others), and an auxiliary database with lots of extra info related to the mapping (which we will use later).

Let's take a look at what we have in the database.

```
anvi-display-contigs-stats T_B_M-contigs.db
```

A little interactive window should pop up in your browser, filled with useful tables of statistics about the co-assembly. At the top you will see a bar chart of the number of annotations to each single-copy core gene (SCGs; which can be annotated by running `anvi-run-hmms`) in the assembly contigs. Here is the chart for the `Bacteria_71` set of bacterial SCGs:

{% include IMAGE width=40 path="/images/binning-popgen-oral-microbiome/T-B-M_hmm_hits_barchart.png" caption="A bar chart of hits to bacterial SCGs in the T-B-M co-assembly" %}

Since we expect that each microbial population should have one copy of each of these SCGs, it looks like we have at least 20 bacterial populations in this co-assmbly (and at most 35). In fact, the most common number of hits to a given bacterial SCG is 25, which is exactly the number of bacterial genomes that anvi'o predicted this assembly to have:

{% include IMAGE width=40 path="/images/binning-popgen-oral-microbiome/T-B-M_num_genomes.png" caption="Predicted number of populations in the T-B-M co-assembly, based on SCG counts" %}

If you want to learn more about how anvi'o estimates the number of genomes, you can click the little `[?]` link in the interactive interface, or just click on [this conveniently-placed link](https://anvio.org/help/main/programs/anvi-display-contigs-stats/#how-do-we-predict-the-number-of-genomes) to the same documentation.

Anyway, we expect to find about **25 bacterial populations** in this co-assembly. We're going to do our best to bin at least some of these populations.

We will be refining metabins that were automatically binned by CONCOCT. Let's check which collections we have available for this co-assembly. Collections of bins are stored in the profile database, so we pass that database as a parameter to `anvi-show-collections-and-bins`:

```
anvi-show-collections-and-bins -p PROFILE.db
```

You should see two collections. One of them is just called `CONCOCT` and it contains over 50 bins:

```
Collection: "CONCOCT"
===============================================
Collection ID ................................: CONCOCT
Number of bins ...............................: 58
Number of splits described ...................: 12,499
Number of contigs described ..................: 12,173
```

These are the binning results from a standard run of CONCOCT, whereby the program did its best to split each individual microbial population into its own bin. As you can see, the number of bins is much larger than the expected number of microbial populations based on single-copy core genes. It is always possible (and perhaps even likely) that we are missing some SCG annotations due to incomplete coverage of less abundant populations. However, it is also possible (and perhaps even very likely) that CONCOCT fragmented some of these population genomes across multiple bins. Which is one reason that we like to use the metabin approach described in [this blog post](https://anvio.org/blog/constrained-binning/) - if you tell CONCOCT to make just a few bins (much less than the number of expected populations), then it is less likely to split sequences from the same population across different bins.

Happily, the second collection in the profile database, `CONCOCT_c10`, is the metabin collection. Shaiber et al. created this collection by running CONCOCT with the `-c 10` parameter, to request 10 bins:

```
Collection: "CONCOCT_c10"
===============================================
Collection ID ................................: CONCOCT_c10
Number of bins ...............................: 10
Number of splits described ...................: 12,499
Number of contigs described ..................: 12,173
Bin names ....................................: Bin_1, Bin_10, Bin_2, Bin_3, Bin_4, Bin_5, Bin_6, Bin_7, Bin_8, Bin_9
```

This is the collection we are going to work on in just a moment.

First, let's just take a look at the co-assembly and its mapping data.

```
anvi-interactive -c T_B_M-contigs.db -p PROFILE.db \
    --title "T-B-M co-assembly"
```

{% include IMAGE width=40 path="/images/binning-popgen-oral-microbiome/T-B-M_coassembly_plain.png" caption="The T-B-M co-assembly in the interactive interface." %}


The profile database comes with pre-loaded settings to make the visualization a bit more digestible. The sample layers are colored according to which individual they come from - in particular, the 5 `T-B-M` samples that were used to make the co-assembly are in dark green. We can see coverage patterns in the samples from the other individuals that were mapped to this co-assembly -- the other individuals' tongue microbiomes include similar populations to those in `T-B-M`'s -- which is good news for us because it means we can use the mapping data from the other samples to help guide our bin refinement.

If you check the 'Items order' label at the top, you will see that the contigs (technically, their splits) are organized according to their shared sequence composition (tetranucleotide frequency) and differential coverage patterns across all 20 samples. The inner dendrogram displays this organization, and we often use this dendrogram to select which contigs to put into a bin -- because we expect that sequences originating from the same genome should have roughly the same composition and follow similar differential coverage patterns.

Just out of curiousity, let's take a look at how the standard CONCOCT bins group the contigs from the co-assembly. You can _either_ load the bin collection `CONCOCT` from the 'Bins' panel in the interface, or close and re-open the interface with `--collection-autoload` in the command:

```
anvi-interactive -c T_B_M-contigs.db -p PROFILE.db \
    --title "T-B-M co-assembly (CONCOCT collection)" \
    --collection-autoload CONCOCT
```

{% include IMAGE width=40 path="/images/binning-popgen-oral-microbiome/T-B-M_coassembly_concoct.png" caption="The regular CONCOCT bins" %}

You can see a lot of mixing between the different bins. There are some sections of the figure where a clade on the inner dendrogram has been sorted into a single bin (such as Bin 2, Bin 10, and Bin 11), but for the most part, the bins are quite fragmented. This is exactly what we wanted to avoid by using the metabin technique.

What do the metabins look like? Again, you can load the collection from the interface, or re-start the interface with a new collection name to load:

```
anvi-interactive -c T_B_M-contigs.db -p PROFILE.db \
    --title "T-B-M co-assembly (CONCOCT_c10 collection)" \
    --collection-autoload CONCOCT_c10
```

{% include IMAGE width=40 path="/images/binning-popgen-oral-microbiome/T-B-M_coassembly_concoct_c10.png" caption="The regular CONCOCT bins" %}

It is a little bit better. Now multiple bacterial populations have been purposefully combined into just a few metabins, and we can see a bit less mixing; ie, the binning follows the dendrogram structure better. And in general, the mixing is not as chaotic -- many clades of the inner dendrogram are sorted into just two bins. We should be able to work with this output to refine a metabin into some high-quality MAGs.

{:.notice}
Why so much mixing? Is CONCOCT doing a really bad job?
Maybe. After all, automatic binning of complex metagenomes [is an extremely difficult task and algorithmic heuristics won't always give us an answer we expect](https://merenlab.org/2020/01/02/visualizing-metagenomic-bins/). But maybe not. It is important to remember that CONCOCT was given only a fraction of the information shown here to base its binning decisions on. It only used differential coverage signal from the 5 dark green samples from T-B-M -- the samples used to make the co-assmbly. Here, we are showing mapping information from 15 extra samples from 3 different individuals, and while it seems clear that those individuals harbor _similar_ populations, there is likely still some variation like large-scale insertions and deletions or hyper-variable regions that could confuse the differential coverage signals. So give CONCOCT a break -- it is trying its best to live up to our unrealistic expectations. :P



