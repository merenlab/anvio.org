---
layout: blog
authors: [ivagljiva, FlorianTrigodet]
title: Metabin refinement and population genetics using human tongue metagenomes
excerpt: "A multi-purpose tutorial based on human oral microbiome data"
date: 2025-03-05
tags: [metagenomics, binning, population genetics, hands-on, beginner]
comments: true
---

Real world datasets are complicated, and it can be difficult to accurately capture this in tutorials due to time and space constraints. The infamous [Infant Gut Tutorial](https://merenlab.org/tutorials/infant-gut/) covers the basics of both binning and population genetics on an extremely simple dataset, which is great for learning but the process and results are not necessarily representative of what one would actually end up doing/seeing in a large-scale 'omics study. Take binning, for example -- if you have dozens of large co-assemblies in your dataset, you won't have the time to do manual binning. And microbial communities containing hundreds of populations rarely show perfectly clean, easy-to-bin patterns of sequence composition and differential coverage. As for population genetics, our existing tutorial shows you how to do the thing, but leaves out some context for the question and important parameter decisions along the way.

With that in mind, we created this tutorial to show a more realistic example of how to work on both metagenomic binning and population genetics in anvi'o, using real-world data from the human oral microbiome as generated in the paper ["Functional and genetic markers of niche partitioning among enigmatic members of the human oral microbiome"](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02195-w) by Shaiber et al (2020). Our goal is to show you the following:

- How to run a read recruitment workflow from many samples to a co-assembly, to generate differential coverage data for binning as well as variant data for population genetics
- How to manually refine a 'metabin' that was automatically generated by Concoct and encompasses several microbial populations into a few smaller yet high-quality bins
- How to set some key parameters in a population genetics analysis that answers a (somewhat) realistic research question

Of course, we are still constrained a little bit by the time and computational resources allowed by a typical tutorial session. We will be working with a smallish co-assembly that is entirely visualizable in the anvi'o interactive interface (unlike many real-world datasets). And it is unlikely that we will actually run the read recruitment workflow in a tutorial setting (but we will give you all the tools to do so in case you want to try it at home).

Without further ado, let's begin :)

## Our dataset and questions

We are taking advantage of the [publicly-available data](https://merenlab.org/data/#niche-partitioning-in-the-human-oral-cavity) from Shaiber et al 2020. This study described a time-series of tongue and plague metagenomes taken from several human individuals, most of which were paired in male-female couples. This sampling strategy is particularly effective for metagenomic binning for the following reasons. First, having multiple samples from a single individual enabled the authors to do co-assembly, which effectively increases the coverage and likelihood of catching low-abundance populations (at the cost of increasing the overall complexity, which can break assembly algorithms). Second, we know that similar populations likely occur across different individuals and it therefore makes sense to map multiple individuals to a given co-assembly to leverage more differential coverage signals -- this was not the strategy used in the Shaiber et al 2020 paper, but it will work quite well for our purposes today.

Here are the questions we want to answer using this dataset:

1. What microbial populations can we recover high-quality metagenome-assembled genomes (MAGs) for by refining 'metabins'?
2. Do the individuals in a couple share similar populations in their oral microbiomes? To put it another way -- can the population variants specific to a given individual help us distinguish between the couples?

In order to answer these questions at a smaller scale, we will be using a subset of the data from the 2020 study - a co-assembly of 5 tongue samples from the time-series of a single individual (the sample named `T-B-M` in the original paper), plus 20 tongue metagenomes to recruit reads from. These include the 5 samples used to make the co-assembly and 5 each from three other individuals: `T-A-F`, `T-A-M`, and `T-B-F`.

For the purposes of this tutorial, we will use the co-assembly that was already generated by the study authors. We will start our binning journey from the results of the automatic binning tool [CONCOCT](https://github.com/BinPro/CONCOCT) ([Alneberg et al 2014](https://www.nature.com/articles/nmeth.3103)) that was already run on the co-assembly. 

In fact, CONCOCT was run twice -- once by specifying `-c 10` to force the tool to create exactly 10 'megabins' (each containing multiple microbial populations), and once without enforcing a number of bins (in which case the tool would try to group sequences from exactly one population in each bin). The 'metabin' strategy is extremely useful for partitioning your large metagenomic datasets into manageable chunks that can be manually refined, and is described elsewhere in [a blog post by Tom and Meren](https://anvio.org/blog/constrained-binning/).

## The mapping workflow

If all we wanted to do was bin refinement, a read recruitment step wouldn't really be necessary here. After all, the automatic binning results were already generated using existing read recruitment results where the 5 time-series samples were mapped against their co-assembly. So we could have just taken the existing, publicly-available contigs and profile databases from [this Figshare](https://figshare.com/articles/dataset/Anvi_o_profiles_per_individual/12217802) and done our bin refinement on that. 

But we want more than that. To answer question (2), we also want to see how the tongue-associated populations from the individual `T-B-M` differ from the tongue-associated populations in the three other individuals (which means we need mapping information from 15 additional samples). And if we are going to map additional samples to the co-assembly anyway, might as well use them to help guide our bin refinement process.

This is probably not a step that you want to run on a laptop. We did it on our high-performance computing cluster (HPC), using the snakemake workflow for metagenomics implemented in `anvi-run-workflow`. We will show you how we set this workflow up, but there is no need for you to run it yourself - you will be able to download the output of the workflow that is required for the rest of the tutorial in the next section.

<details markdown="1"><summary>Show/Hide You want to run the mapping workflow yourself? Here is how to get the short read data. </summary>

To get the metagenome samples, open the [SRA Run Selector for the BioProject PRJNA625082](https://www.ncbi.nlm.nih.gov/Traces/study/?query_key=2&WebEnv=MCID_67c08dce0e84b53f0ea4df2a&f=env_medium_sam_ss%3An&o=acc_s%3Aa) in your favorite web browser. In the filters on the left side, filter for 'host_tissue_sampled' = 'Tongue' and 'Assay_Type' = 'WGS' to get all the tongue samples. Then downloaded the metadata table as a CSV. You can then select only the time-series samples from our 4 individuals of interest by running the following code on the metadata table in your terminal:

```
# keep only accession and sample name
cut -d ',' -f 1,45 Oral_Microbiome_SraRunTable.csv | tr ',' '\t' > oral_samples.txt
# search for sample names of interest and extract to a new file
for n in T_B_M T_B_F T_A_F T_A_M; do \
  grep $n oral_samples.txt >> oral_samps_to_download.txt ; \
done
# extract only the accession for the samples of interest
cut -f 1 oral_samps_to_download.txt > SRA_accession_list.txt
```

This creates a file of SRA accession numbers that becomes the input to the SRA download workflow. You should put that `SRA_accession_list.txt` file wherever you want to download those samples (for instance, on your HPC), and then get a default config file for the workflow:

```
anvi-run-workflow -w sra_download --get-default-config download.json
```

Feel free to open that config and change the parameters (for instance, number of threads) for each rule according to the computational resources available to you.

When you are ready to download the samples, running the workflow is as simple as this:

```
anvi-run-workflow -w sra_download -c download.json
```

If you are running on a cluster, you may want to use the `--additional-params` (or `-A`) flag to pass cluster-specific scheduler commands and resource limits to snakemake using flags such as `--cluster`, `--resources`, `--jobs`, etc.

Once the workflow is done, you should have a folder of FASTQ files for 20 metagenome samples. And then you'll be ready to run the mapping workflow!

</details>

So, how did we run the mapping workflow?

First, we got our reference metagenome co-assembly for individual T-B-M by downloading the corresponding contigs and profile database from [this Figshare link](https://figshare.com/articles/dataset/Anvi_o_profiles_per_individual/12217802). Then, we extracted the FASTA file of the contigs, as well as both existing CONCOCT collections that were already stored in the profile database:

```
# extract le data
tar -xvf T-B-M.tar.gz

# update the dbs to match your current anvi'o version
anvi-migrate --migrate-safe T-B-M/*.db

# get our reference FASTA file
anvi-export-contigs -c T-B-M/CONTIGS.db -o T_B_M-contigs.fa

# export the metabin collection
anvi-export-collection -p T-B-M/PROFILE.db -C CONCOCT_c10 -O CONCOCT_c10

# export the standard CONCOCT collection
anvi-export-collection -p T-B-M/PROFILE.db -C CONCOCT -O CONCOCT
```

We will use the FASTA file of the co-assembly contigs as the reference in our mapping workflow. The mapping workflow will create a new contigs database out of this FASTA file, and associate the mapping data to that new contigs database via a new (merged) profile database containing the mapping info from all 20 of our samples. Because we'll get a fresh profile database that won't have any of the existing collection information in it, we will need to import the collection data (using the collection files we just exported) so that we can do our bin refinement from there. Luckily, because we exported our reference FASTA directly from the contigs database, the split names in the exported collection files will match to the names in the new contigs database. But we are getting ahead of ourselves. First we actually have to run the workflow.

On our HPC, we navigated to the folder where we downloaded the 20 metagenome samples that we want to map against this co-assembly. We created a samples-txt file that describes the paths to each of the samples (you can see the format [here](https://anvio.org/help/main/artifacts/samples-txt/)). Then we generated a fasta-txt file with the path to our reference co-assembly FASTA, and we got a default config file for the metagenomics workflow:

```
echo -e "name\tpath\nT_B_M\tT_B_M-contigs.fa" > fasta.txt
anvi-run-workflow -w metagenomics --get-default-config map.json
```

After modifying the config file a bit to turn off unnecessary rules (like functional annotation) and increase the number of threads for each rule as much as our cluster could handle, we did a dry run (using the `-n` dry run flag and `-q` quiet flag passed directly to snakemake using `-A` or `--additional-params`) to see what the workflow would actually run:
```
anvi-run-workflow -w metagenomics -c map.json -A -n -q
```

And it showed us the following list of jobs:
```
job                                  count
---------------------------------  -------
annotate_contigs_database                1
anvi_gen_contigs_database                1
anvi_init_bam                           20
anvi_merge                               1
anvi_profile                            20
anvi_run_hmms                            1
anvi_run_scg_taxonomy                    1
bowtie                                  20
bowtie_build                             1
gen_qc_report                            1
gzip_fastqs                             40
import_percent_of_reads_mapped          20
iu_filter_quality_minoche               20
iu_gen_configs                           1
metagenomics_workflow_target_rule        1
samtools_view                           20
total                                  169
```

Everything looked okay in the dry run. We have 20 samples, so it makes sense that we are doing all the sample-specific jobs 20 times (the only exception, `gzip_fastqs`, works individually on R1 and R2 files, so it runs twice per samples).

We very confidently started the workflow using the command:

```
anvi-run-workflow -w metagenomics -c map.json
```

And after it finished successfully, we took the resulting contigs database and merged profile database, and we imported the original collections of CONCOCT bins that we extracted previously:

```
anvi-import-collection -p PROFILE.db -C CONCOCT_c10 CONCOCT_c10.txt -c T_B_M-contigs.db
anvi-import-collection -c T_B_M-contigs.db -p PROFILE.db -C CONCOCT CONCOCT.txt
```

These are the databases that were put into the datapack for this tutorial, which you can download in the next section.


